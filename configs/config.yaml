# Optimized configuration for ResNet-50 on high-performance GPU
# Based on EDA: 1,570 samples, 17 classes, imbalance ratio 2.2:1

# WandB Configuration
wandb:
  enabled: true
  project: "document-classifier"
  entity: null
  name: None
  username: wchoi189
  tags: ["resnet50", "document-classification", "optimized", "high-perf-gpu"]
  notes: "ResNet-50 for 17-class document classification with 2.2:1 imbalance"

  # Logging settings
  log_frequency: 10        # More frequent logging for better monitoring
  log_images: true
  log_model: false         # Save best models as artifacts
  log_gradients: false
  log_confusion_matrix: true

  # Advanced features
  watch_model: true
  log_code: true

# Basic settings
seed: 42
device: 'cuda'

# Data configuration - optimized for your dataset specs
data:
  root_dir: "data/raw"
  csv_file: "data/raw/metadata/train.csv"
  meta_file: "data/raw/metadata/meta.csv"
  image_size: 224        # Increased from 224 - better for document images
  val_size: 0.2
  num_workers: 0         # Increased for high-perf GPU (adjust based on CPU cores)
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  use_document_augmentation: true  # Enable for better generalization

# Model configuration
model:
  name: "resnet50"
  pretrained: true

# Training configuration - optimized for high-performance GPU
train:
  epochs: 50             # Increased for better convergence
  batch_size: 128        # Larger batch size for high-perf GPU
  optimizer: 'AdamW'     # Better than Adam for most cases
  loss: 'CrossEntropyLoss'

  # Optimizer hyperparameters - tuned for ResNet-50
  learning_rate: 0.001   # Higher LR with larger batch size
  weight_decay: 0.01     # Standard for AdamW

  # Mixed precision for faster training on modern GPUs
  mixed_precision: true

  # Learning rate scheduler
  scheduler: 'CosineAnnealingWarmRestarts'
  scheduler_params:
    T_0: 10              # Restart every 10 epochs
    T_mult: 2            # Double restart period each time
    eta_min: 0.00001     # Minimum learning rate

  # Early stopping configuration
  early_stopping:
    patience: 8          # Increased patience for longer training
    metric: 'val_f1'
    mode: 'max'

# Advanced training optimizations
optimization:
  # Gradient clipping
  gradient_clip_norm: 1.0
  
  # Label smoothing for better generalization
  label_smoothing: 0.1
  
  # Warmup for stable training start
  warmup_epochs: 3
  warmup_factor: 0.1

# Data augmentation settings
augmentation:
  # Basic augmentations
  rotation_limit: 15
  brightness_contrast_prob: 0.3
  noise_prob: 0.2
  
  # Document-specific augmentations
  jpeg_quality_range: [70, 95]
  blur_prob: 0.2
  shadow_prob: 0.1

# Logging and checkpointing
logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  log_interval: 10        # More frequent logging
  memory_logging: true
  
  # Save checkpoints more frequently
  save_freq: 10           # Save every 5 epochs
  save_best_only: false  # Keep multiple checkpoints

# Resource utilization settings for high-performance GPU
gpu_optimization:
  # Enable optimizations
  torch_compile: true    # PyTorch 2.0 compilation (if available)
  channels_last: true    # Memory format optimization
  
  # Memory management
  empty_cache_freq: 20   # Clear cache every 20 batches
  pin_memory: true
  non_blocking: true

# Validation and testing
validation:
  # More comprehensive validation
  compute_class_metrics: true
  save_predictions: true
  plot_curves: true

# Model ensemble settings (optional for final submission)
# ensemble:
#   enabled: false
#   models: ["resnet50", "efficientnet_b3"]
#   weights: [0.6, 0.4]

# Class imbalance handling
class_balance:
  # Given your 2.2:1 imbalance ratio
  use_weighted_loss: true
  focal_loss_alpha: 0.25
  focal_loss_gamma: 2.0
  
  # Sampling strategy
  use_balanced_sampler: false  # Try both true/false
  oversample_minority: false