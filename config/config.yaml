# 기본 설정
seed: 42
device: 'cuda' # 'cuda' 또는 'cpu'

# # 데이터 설정 Folder based
# data:
#   root_dir: "data/dataset"
#   # image_size: 128 # Was 224, reduce memory by ~75%
#   image_size: 224 # ResNet34 모델에 맞는 이미지 크기
#   # image_size: 512 # Instead of 224 - less extreme downsampling 
#   val_size: 0.2
#   num_workers: 4
#   mean: [0.485, 0.456, 0.406]
#   std: [0.229, 0.224, 0.225]
#   use_document_augmentation: False

# 데이터 설정 CSV based
data:
  root_dir: "data/dataset"
  csv_file: "data/dataset/train.csv"
  meta_file: "data/dataset/meta.csv"
  image_size: 224
  val_size: 0.2
  num_workers: 0  # Keep at 0 for CUDA compatibility
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  use_document_augmentation: false

  # 추가: 데이터 증강 설정
  # augmentations:
  #   use_blur: true
  #   blur_params:
  #     blur_limit: [3, 7]  # 최소값을 3으로 설정
  #     sigma_limit: [0.1, 2.0]  # 최소값을 0보다 크게 설정
      
# 모델 설정
model:
  name: "resnet50"
  # name: "resnet34" # Was resnet50, ~30% less memory
  pretrained: true

# 학습 설정
train:
  # use_amp: true # 자동 혼합 정밀도 사용 여부
  epochs: 20
  batch_size: 16 # Was 32, reduce by half
  optimizer: 'AdamW' # 'Adam', 'AdamW', 'SGD' 등 torch.optim에 있는 옵티마이저
  loss: 'CrossEntropyLoss' # 'CrossEntropyLoss', 'FocalLoss' 등 (FocalLoss는 별도 구현 필요)

  # 옵티마이저 하이퍼파라미터
  learning_rate: 0.0001
  weight_decay: 0.0001

  # 스케줄러 설정 (비어있으면 사용 안 함)
  # scheduler: 'CosineAnnealingLR' # 'CosineAnnealingLR', 'ReduceLROnPlateau' 등
  # scheduler_params:
  #   T_max: 20 # for CosineAnnealingLR
  #   eta_min: 0.000001 # for CosineAnnealingLR
  #   # mode: 'min' # for ReduceLROnPlateau
  #   # factor: 0.1 # for ReduceLROnPlateau
  #   # patience: 3 # for ReduceLROnPlateau

  # 조기 종료 설정 (train 섹션 안으로 이동)
  early_stopping:
    patience: 5
    metric: 'val_f1' # 'val_loss', 'val_acc', 'val_f1'
    mode: 'max' # val_loss의 경우 'min', 나머지는 'max'

# 로깅 및 저장 경로
logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  log_interval: 10 # N 배치마다 학습 손실 로그 출력
  memory_logging: true # 메모리 사용량 로그 출력 여부